{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Million Song Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description: \n",
    "   A freely-available collection of audio features and metadata for a million contemporary popular music tracks.\n",
    "   \n",
    "   Subset Size: 1.8 GB / 10,000 songs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to get the tempo, meter, and loundess from HDF5 files for each song:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tables\n",
    "\n",
    "# open an hdf5 file\n",
    "\n",
    "\n",
    "def open_h5_file_read(h5filename):\n",
    "\n",
    "    return tables.open_file(h5filename, mode='r')\n",
    "\n",
    "# get duration from an hdf5 file\n",
    "\n",
    "\n",
    "def get_duration(h5, songidx=0):\n",
    "\n",
    "    return h5.root.analysis.songs.cols.duration[songidx]\n",
    "\n",
    "# get loudness from an hdf5 file\n",
    "\n",
    "\n",
    "def get_loudness(h5, songidx=0):\n",
    "\n",
    "    return h5.root.analysis.songs.cols.loudness[songidx]\n",
    "\n",
    "# get tempo from an hdf5 file\n",
    "\n",
    "\n",
    "def get_tempo(h5, songidx=0):\n",
    "\n",
    "    return h5.root.analysis.songs.cols.tempo[songidx]\n",
    "\n",
    "# get pitches array from an hdf5 file\n",
    "\n",
    "\n",
    "def get_segments_pitches(h5, songidx=0):\n",
    "\n",
    "    if h5.root.analysis.songs.nrows == songidx + 1:\n",
    "        return h5.root.analysis.segments_pitches[h5.root.analysis.songs.cols.idx_segments_pitches[songidx]:, :]\n",
    "    return h5.root.analysis.segments_pitches[h5.root.analysis.songs.cols.idx_segments_pitches[songidx]:\n",
    "                                             h5.root.analysis.songs.cols.idx_segments_pitches[songidx + 1], :]\n",
    "\n",
    "# get timbre array from an hdf5 file\n",
    "\n",
    "\n",
    "def get_segments_timbre(h5, songidx=0):\n",
    "\n",
    "    if h5.root.analysis.songs.nrows == songidx + 1:\n",
    "        return h5.root.analysis.segments_timbre[h5.root.analysis.songs.cols.idx_segments_timbre[songidx]:, :]\n",
    "    return h5.root.analysis.segments_timbre[h5.root.analysis.songs.cols.idx_segments_timbre[songidx]:\n",
    "                                            h5.root.analysis.songs.cols.idx_segments_timbre[songidx + 1], :]\n",
    "\n",
    "# get bars start array from an hdf5 file\n",
    "\n",
    "\n",
    "def get_bars_start(h5, songidx=0):\n",
    "\n",
    "    if h5.root.analysis.songs.nrows == songidx + 1:\n",
    "        return h5.root.analysis.bars_start[h5.root.analysis.songs.cols.idx_bars_start[songidx]:]\n",
    "    return h5.root.analysis.bars_start[h5.root.analysis.songs.cols.idx_bars_start[songidx]:\n",
    "                                       h5.root.analysis.songs.cols.idx_bars_start[songidx + 1]]\n",
    "\n",
    "# get beats start array from an hdf5 file\n",
    "\n",
    "\n",
    "def get_beats_start(h5, songidx=0):\n",
    "\n",
    "    if h5.root.analysis.songs.nrows == songidx + 1:\n",
    "        return h5.root.analysis.beats_start[h5.root.analysis.songs.cols.idx_beats_start[songidx]:]\n",
    "    return h5.root.analysis.beats_start[h5.root.analysis.songs.cols.idx_beats_start[songidx]:\n",
    "                                        h5.root.analysis.songs.cols.idx_beats_start[songidx + 1]]\n",
    "\n",
    "# identify meter from an hdf5 file\n",
    "\n",
    "\n",
    "def get_meter(h5, sonidx=0):\n",
    "\n",
    "    beats = get_beats_start(h5)\n",
    "    bar = get_bars_start(h5)\n",
    "    meter = int(beats.shape[0] / bar.shape[0])\n",
    "    return meter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Lyrics of 10,000 songs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description:\n",
    "   Web Scraped from www.azlyrics.com and stored in a csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to perform topic modeling and sentiment analysis for each song:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "# Perform topic modeling\n",
    "\n",
    "\n",
    "def get_topics(song_id):\n",
    "    lyrics = df.iloc[song_id]\n",
    "    text = ''.join(np.array(lyrics).tolist())\n",
    "    striptext = text_pop.replace('\\n\\n', ' ').replace('\\n', ' ')\n",
    "\n",
    "    sentences = sent_tokenize(striptext)\n",
    "    texts = [[word for word in sentence.lower().split(\n",
    "    ) if word not in STOPWORDS and word.isalnum()] for sentence in sentences]\n",
    "\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "    num_topics = 1\n",
    "    passes = 5\n",
    "\n",
    "    lda = LdaModel(corpus, id2word=dictionary, num_topics=num_topics, passes=5)\n",
    "    topic = lda.print_topics(num_words=passes)[0]\n",
    "\n",
    "    return topic\n",
    "\n",
    "# Perform sentment analysis\n",
    "\n",
    "\n",
    "def get_pos_neg_words():\n",
    "    def get_words(url):\n",
    "        import requests\n",
    "        words = requests.get(url).content.decode('latin-1')\n",
    "        word_list = words.split('\\n')\n",
    "        index = 0\n",
    "        while index < len(word_list):\n",
    "            word = word_list[index]\n",
    "            if ';' in word or not word:\n",
    "                word_list.pop(index)\n",
    "            else:\n",
    "                index += 1\n",
    "        return word_list\n",
    "\n",
    "    # Get lists of positive and negative words\n",
    "    p_url = 'http://ptrckprry.com/course/ssd/data/positive-words.txt'\n",
    "    n_url = 'http://ptrckprry.com/course/ssd/data/negative-words.txt'\n",
    "    positive_words = get_words(p_url)\n",
    "    negative_words = get_words(n_url)\n",
    "    return positive_words, negative_words\n",
    "\n",
    "\n",
    "def do_pos_neg_sentiment_analysis(song_id):\n",
    "    positive_words, negative_words = get_pos_neg_words()\n",
    "    lyrics = df.iloc[song_id]\n",
    "    text = ''.join(np.array(lyrics).tolist())\n",
    "    striptext = text_pop.replace('\\n\\n', ' ').replace('\\n', ' ')\n",
    "    sentences = sent_tokenize(striptext)\n",
    "    texts = [[word for word in sentence.lower().split(\n",
    "    ) if word not in STOPWORDS and word.isalnum()] for sentence in sentences]\n",
    "\n",
    "    result = list()\n",
    "    cpos = cneg = 0\n",
    "    for word in word_tokenize(texts):\n",
    "        if word in positive_words:\n",
    "                cpos += 1\n",
    "        if word in negative_words:\n",
    "                cneg+=1\n",
    "    result = ( cpos/len(word_tokenize(texts))*100,\n",
    "                            cneg/len(word_tokenize(texts))*100)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform k-means clustering on datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input: a matrix X which contains 5 features\n",
    " tempo, loudness, meter, positive ratio for lyrics, negative ratio for lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output: \n",
    " labels for 10,000 songs range from 1 to 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Number of clusters\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "# Fitting the input data\n",
    "kmeans = kmeans.fit(X)\n",
    "# Getting the cluster labels\n",
    "labels = kmeans.predict(X)\n",
    "# Centroid values\n",
    "centroids = kmeans.cluster_centers_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
